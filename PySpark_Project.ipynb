{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7u4y4gbx5afb"
      },
      "outputs": [],
      "source": [
        "!pip install -q pyspark pyarrow pandas\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = (SparkSession.builder\n",
        "         .appName(\"RetailPulse_Colab\")          # appears in the Spark UI/job name\n",
        "         .master(\"local[*]\")                    # use all local CPU cores on the VM\n",
        "         .config(\"spark.sql.shuffle.partitions\", \"4\")   # fewer shuffle tasks for tiny data\n",
        "         .config(\"spark.sql.adaptive.enabled\", \"true\")  # AQE: smarter plans at runtime\n",
        "         .getOrCreate())\n",
        "\n",
        "print(\"Spark version:\", spark.version)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNVFrzYk7Vkq",
        "outputId": "2fa214c4-1cd6-4f69-90a5-e7f7086d425a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark version: 3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#creating sample raw csv files\n",
        "import pathlib, textwrap\n",
        "\n",
        "RAW = pathlib.Path(\"data/raw\"); RAW.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "(RAW / \"transactions.csv\").write_text(textwrap.dedent(\"\"\"\n",
        "order_id,customer_id,order_ts,sku,qty,price,country,channel\n",
        "1001,C001,2025-07-01 10:05:10,SKU_01,1,19.99,NO,web\n",
        "1001,C001,2025-07-01 10:05:10,SKU_02,2,5.00,NO,web\n",
        "1002,C002,2025-07-02 13:22:45,SKU_03,1,49.00,US,app\n",
        "1003,C003,2025-07-03 08:01:00,SKU_01,3,19.99,CA,web\n",
        "1004,C001,2025-07-10 19:30:12,SKU_04,1,129.00,NO,app\n",
        "1005,C004,2025-07-12 15:11:59,SKU_02,4,5.00,NO,web\n",
        "\"\"\").strip()+\"\\n\")\n",
        "\n",
        "(RAW / \"customers.csv\").write_text(textwrap.dedent(\"\"\"\n",
        "customer_id,signup_date,loyalty_tier,region\n",
        "C001,2024-11-20,Gold,Nordics\n",
        "C002,2025-01-05,Silver,North America\n",
        "C003,2023-09-12,Bronze,North America\n",
        "C004,2025-03-18,Silver,Nordics\n",
        "\"\"\").strip()+\"\\n\")\n",
        "\n",
        "print(\"Wrote:\", RAW / \"transactions.csv\")\n",
        "print(\"Wrote:\", RAW / \"customers.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gj2gODBp7gWh",
        "outputId": "d00a6fb2-8caa-43ea-d403-9e32ac049c8d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote: data/raw/transactions.csv\n",
            "Wrote: data/raw/customers.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#explicit schema\n",
        "from pyspark.sql import types as T, functions as F\n",
        "\n",
        "schema = T.StructType([\n",
        "    T.StructField(\"order_id\",    T.IntegerType()),\n",
        "    T.StructField(\"customer_id\", T.StringType()),\n",
        "    T.StructField(\"order_ts\",    T.TimestampType()),\n",
        "    T.StructField(\"sku\",         T.StringType()),\n",
        "    T.StructField(\"qty\",         T.IntegerType()),\n",
        "    T.StructField(\"price\",       T.DoubleType()),\n",
        "    T.StructField(\"country\",     T.StringType()),\n",
        "    T.StructField(\"channel\",     T.StringType()),\n",
        "])\n",
        "\n",
        "tx = (spark.read.format(\"csv\")\n",
        "      .option(\"header\", True)\n",
        "      .schema(schema)                               # <- important\n",
        "      .load(\"data/raw/transactions.csv\")\n",
        "      .withColumn(\"order_date\", F.to_date(\"order_ts\")))  # for partitioning/pruning later\n",
        "\n",
        "tx.show(truncate=False)\n",
        "tx.printSchema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JI0_sBp17ixP",
        "outputId": "19866530-b98b-4864-bad2-b6c827adcfec"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----------+-------------------+------+---+-----+-------+-------+----------+\n",
            "|order_id|customer_id|order_ts           |sku   |qty|price|country|channel|order_date|\n",
            "+--------+-----------+-------------------+------+---+-----+-------+-------+----------+\n",
            "|1001    |C001       |2025-07-01 10:05:10|SKU_01|1  |19.99|NO     |web    |2025-07-01|\n",
            "|1001    |C001       |2025-07-01 10:05:10|SKU_02|2  |5.0  |NO     |web    |2025-07-01|\n",
            "|1002    |C002       |2025-07-02 13:22:45|SKU_03|1  |49.0 |US     |app    |2025-07-02|\n",
            "|1003    |C003       |2025-07-03 08:01:00|SKU_01|3  |19.99|CA     |web    |2025-07-03|\n",
            "|1004    |C001       |2025-07-10 19:30:12|SKU_04|1  |129.0|NO     |app    |2025-07-10|\n",
            "|1005    |C004       |2025-07-12 15:11:59|SKU_02|4  |5.0  |NO     |web    |2025-07-12|\n",
            "+--------+-----------+-------------------+------+---+-----+-------+-------+----------+\n",
            "\n",
            "root\n",
            " |-- order_id: integer (nullable = true)\n",
            " |-- customer_id: string (nullable = true)\n",
            " |-- order_ts: timestamp (nullable = true)\n",
            " |-- sku: string (nullable = true)\n",
            " |-- qty: integer (nullable = true)\n",
            " |-- price: double (nullable = true)\n",
            " |-- country: string (nullable = true)\n",
            " |-- channel: string (nullable = true)\n",
            " |-- order_date: date (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#land raw CSV, then curate to Parquet partitioned by date for pruning\n",
        "from pathlib import Path\n",
        "\n",
        "OUT = Path(\"data/processed/tx_parquet\"); OUT.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "(tx.repartition(\"order_date\")                 # line data shuffled by day before write\n",
        "   .write.mode(\"overwrite\")\n",
        "   .partitionBy(\"order_date\")                 # creates folders like order_date=2025-07-01/\n",
        "   .parquet(str(OUT)))\n",
        "\n",
        "print(\"Wrote:\", OUT)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-eFyd5c8DXd",
        "outputId": "735a3387-9e40-44ab-f27d-79b6d57e005c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote: data/processed/tx_parquet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#daily KPI: revenue by day\n",
        "tx = spark.read.parquet(str(OUT)).withColumn(\"line_total\", F.col(\"qty\") * F.col(\"price\"))\n",
        "\n",
        "daily_rev = (tx.withColumn(\"order_day\", F.to_date(\"order_ts\"))\n",
        "               .groupBy(\"order_day\")\n",
        "               .agg(F.round(F.sum(\"line_total\"), 2).alias(\"revenue\"))\n",
        "               .orderBy(\"order_day\"))\n",
        "\n",
        "daily_rev.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXp6gLn48MFm",
        "outputId": "c7572348-62ca-4fce-8237-235f494e0fac"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------+\n",
            "| order_day|revenue|\n",
            "+----------+-------+\n",
            "|2025-07-01|  29.99|\n",
            "|2025-07-02|   49.0|\n",
            "|2025-07-03|  59.97|\n",
            "|2025-07-10|  129.0|\n",
            "|2025-07-12|   20.0|\n",
            "+----------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#window function - top 3 SKUs by country\n",
        "from pyspark.sql import Window as W\n",
        "\n",
        "totals = tx.groupBy(\"country\",\"sku\").agg(F.sum(\"qty\").alias(\"qty\"))\n",
        "win = W.partitionBy(\"country\").orderBy(F.desc(\"qty\"))\n",
        "top_skus = (totals\n",
        "            .withColumn(\"rk\", F.row_number().over(win))   # rank SKUs inside each country\n",
        "            .where(F.col(\"rk\") <= 3)\n",
        "            .drop(\"rk\"))\n",
        "\n",
        "top_skus.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b24J51g8cDz",
        "outputId": "ad093e5c-5bcf-4997-ee6e-75703b4a1f1e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------+---+\n",
            "|country|   sku|qty|\n",
            "+-------+------+---+\n",
            "|     CA|SKU_01|  3|\n",
            "|     NO|SKU_02|  6|\n",
            "|     NO|SKU_01|  1|\n",
            "|     NO|SKU_04|  1|\n",
            "|     US|SKU_03|  1|\n",
            "+-------+------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#RFM features (Recency, Frequency, Monetary)\n",
        "# De-duplicate to 1 row per (order_id, customer_id, order_ts)\n",
        "tx_orders = (tx.select(\"order_id\",\"customer_id\",\"order_ts\",\"line_total\")\n",
        "               .dropDuplicates([\"order_id\",\"customer_id\",\"order_ts\"]))\n",
        "\n",
        "# Aggregate per customer\n",
        "rfm = (tx_orders.groupBy(\"customer_id\")\n",
        "        .agg(F.max(\"order_ts\").alias(\"last_order_ts\"),\n",
        "             F.countDistinct(\"order_id\").alias(\"frequency\"),\n",
        "             F.sum(\"line_total\").alias(\"monetary\")))\n",
        "\n",
        "# Recency in days = (now - last_order_ts) / 86400\n",
        "rfm = (rfm\n",
        "       .withColumn(\"today\", F.to_timestamp(F.lit(\"2025-08-17 00:00:00\")))\n",
        "       .withColumn(\"recency_days\",\n",
        "                   (F.col(\"today\").cast(\"long\") - F.col(\"last_order_ts\").cast(\"long\"))/86400.0)\n",
        "       .drop(\"today\"))\n",
        "\n",
        "rfm.orderBy(\"customer_id\").show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUolcxMC8mNh",
        "outputId": "42211f33-54c5-454d-ef6c-901e1e8ce56f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------------+---------+--------+-----------------+\n",
            "|customer_id|last_order_ts      |frequency|monetary|recency_days     |\n",
            "+-----------+-------------------+---------+--------+-----------------+\n",
            "|C001       |2025-07-10 19:30:12|2        |148.99  |37.18736111111111|\n",
            "|C002       |2025-07-02 13:22:45|1        |49.0    |45.44253472222222|\n",
            "|C003       |2025-07-03 08:01:00|1        |59.97   |44.66597222222222|\n",
            "|C004       |2025-07-12 15:11:59|1        |20.0    |35.36667824074074|\n",
            "+-----------+-------------------+---------+--------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#join customer attributes for BI and profiling\n",
        "cust = (spark.read.format(\"csv\").option(\"header\", True)\n",
        "        .load(\"data/raw/customers.csv\")\n",
        "        .withColumn(\"signup_date\", F.to_date(\"signup_date\")))\n",
        "\n",
        "rfm_joined = (rfm.join(cust, \"customer_id\", \"left\")\n",
        "                .select(\"customer_id\",\"recency_days\",\"frequency\",\"monetary\",\"loyalty_tier\",\"region\"))\n",
        "\n",
        "rfm_joined.show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kb5h8Mcc8tcv",
        "outputId": "962f2e6e-dc71-424f-da8a-c98ee3b35213"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----------------+---------+--------+------------+-------------+\n",
            "|customer_id|recency_days     |frequency|monetary|loyalty_tier|region       |\n",
            "+-----------+-----------------+---------+--------+------------+-------------+\n",
            "|C001       |37.18736111111111|2        |148.99  |Gold        |Nordics      |\n",
            "|C004       |35.36667824074074|1        |20.0    |Silver      |Nordics      |\n",
            "|C002       |45.44253472222222|1        |49.0    |Silver      |North America|\n",
            "|C003       |44.66597222222222|1        |59.97   |Bronze      |North America|\n",
            "+-----------+-----------------+---------+--------+------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#customer segmentation with KMeans\n",
        "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
        "from pyspark.ml.clustering import KMeans\n",
        "\n",
        "# Assemble & scale features (scaling avoids “big-number” bias)\n",
        "feats = rfm_joined.fillna(0).select(\"customer_id\",\"recency_days\",\"frequency\",\"monetary\")\n",
        "va = VectorAssembler(inputCols=[\"recency_days\",\"frequency\",\"monetary\"], outputCol=\"features_raw\")\n",
        "scaled = (StandardScaler(inputCol=\"features_raw\", outputCol=\"features\", withStd=True, withMean=True)\n",
        "          .fit(va.transform(feats))\n",
        "          .transform(va.transform(feats)))\n",
        "\n",
        "# Train KMeans and assign clusters\n",
        "kmeans = KMeans(k=3, seed=42, featuresCol=\"features\", predictionCol=\"cluster\")\n",
        "model = kmeans.fit(scaled)\n",
        "clusters = model.transform(scaled).select(\"customer_id\",\"cluster\")\n",
        "clusters.show()\n",
        "\n",
        "# Cluster profile (avg R/F/M per cluster)\n",
        "profile = (clusters.join(rfm_joined, \"customer_id\")\n",
        "                 .groupBy(\"cluster\")\n",
        "                 .agg(F.round(F.avg(\"recency_days\"),1).alias(\"avg_recency\"),\n",
        "                      F.round(F.avg(\"frequency\"),2).alias(\"avg_freq\"),\n",
        "                      F.round(F.avg(\"monetary\"),2).alias(\"avg_monetary\"),\n",
        "                      F.count(\"*\").alias(\"customers\")))\n",
        "profile.orderBy(\"cluster\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xO_dbXId9AQq",
        "outputId": "bafd2e8d-f345-4452-f25f-26e394ee8860"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------+\n",
            "|customer_id|cluster|\n",
            "+-----------+-------+\n",
            "|       C001|      1|\n",
            "|       C004|      2|\n",
            "|       C002|      0|\n",
            "|       C003|      0|\n",
            "+-----------+-------+\n",
            "\n",
            "+-------+-----------+--------+------------+---------+\n",
            "|cluster|avg_recency|avg_freq|avg_monetary|customers|\n",
            "+-------+-----------+--------+------------+---------+\n",
            "|      0|       45.1|     1.0|       54.49|        2|\n",
            "|      1|       37.2|     2.0|      148.99|        1|\n",
            "|      2|       35.4|     1.0|        20.0|        1|\n",
            "+-------+-----------+--------+------------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#7 day revenue forecast\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "# Make a supervised dataset: trend index + day-of-week\n",
        "dr = daily_rev.withColumn(\"day_index\", F.row_number().over(Window.orderBy(\"order_day\")) - 1)\n",
        "dr = dr.withColumn(\"dow\", F.dayofweek(\"order_day\"))  # 1..7\n",
        "\n",
        "va = VectorAssembler(inputCols=[\"day_index\", \"dow\"], outputCol=\"features\")\n",
        "train_ml = va.transform(dr.select(\"revenue\",\"day_index\",\"dow\")).select(\"features\",\"revenue\")\n",
        "\n",
        "lr = LinearRegression(featuresCol=\"features\", labelCol=\"revenue\")\n",
        "lr_model = lr.fit(train_ml)\n",
        "\n",
        "# Build 7 future rows (toy example)\n",
        "max_idx = dr.agg(F.max(\"day_index\")).first()[0]\n",
        "max_dow = dr.agg(F.max(\"dow\")).first()[0]\n",
        "\n",
        "future = spark.range(1, 8).withColumn(\"future_index\", F.col(\"id\") + max_idx).drop(\"id\")\n",
        "future = (future.withColumn(\"dow\", ((F.lit(max_dow) + F.col(\"future_index\")) % 7) + 1)\n",
        "                .withColumn(\"day_index\", F.col(\"future_index\")))\n",
        "\n",
        "pred = (lr_model.transform(va.transform(future.select(\"day_index\",\"dow\")))\n",
        "                 .withColumnRenamed(\"prediction\",\"forecast_revenue\")\n",
        "                 .select(\"day_index\",\"dow\",\"forecast_revenue\"))\n",
        "\n",
        "pred.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZ_7hS_x9MhZ",
        "outputId": "15b2890f-1bca-4ffe-e2fb-7775e2d339f6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---+------------------+\n",
            "|day_index|dow|  forecast_revenue|\n",
            "+---------+---+------------------+\n",
            "|        5|  6|215.75371428571492|\n",
            "|        6|  7|212.41200000000055|\n",
            "|        7|  1| 863.1302857142884|\n",
            "|        8|  2| 859.7885714285742|\n",
            "|        9|  3| 856.4468571428599|\n",
            "|       10|  4| 853.1051428571457|\n",
            "|       11|  5| 849.7634285714312|\n",
            "+---------+---+------------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}